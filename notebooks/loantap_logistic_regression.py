# -*- coding: utf-8 -*-
"""LoanTap Logistic Regression

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a8O0PFq004cr52KID-ttfLT9TAs_z8J3
"""

pip install category_encoders

import pandas as pd
import numpy as np
import seaborn as sbn
import matplotlib.pyplot as plt
import category_encoders as ce
from sklearn.preprocessing import OrdinalEncoder,OneHotEncoder,  LabelEncoder
from sklearn.preprocessing import StandardScaler, MinMaxScaler

from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

from lightgbm import LGBMClassifier, LGBMRegressor
import warnings
warnings.filterwarnings('ignore')
from sklearn.impute import SimpleImputer
import statsmodels.api as sm
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from scipy import stats
from statsmodels.graphics.gofplots import qqplot
from scipy.stats import pearsonr
from scipy.stats import shapiro



#!gdown https://drive.google.com/file/d/1ZPYj7CZCfxntE8p2Lze_4QO4MyEOy6_d/view?usp=sharing

#dj = pd.read_csv('logistic_regression.csv')
#dj.head()

#To see all columns in dataset
pd.set_option('display.max_columns', None)

dl=pd.read_csv("/content/sample_data/logistic_regression.csv")
dl.head()

dl.info()

dl.describe()

dl.shape

dl['mort_acc'] = dl['mort_acc'].astype(object)

dl['pub_rec_bankruptcies'].unique()

dl['revol_util'].unique()

C = dl.columns
missing_value = pd.DataFrame({
    'Missing Value': dl.isnull().sum(),
    'Percentage': (dl.isnull().sum() / len(dl))*100,
    'Unique Value': dl.nunique(),
    'data types': dl.dtypes,
})
missing_value[missing_value['Missing Value']>0].sort_values(by='Percentage', ascending=False)

successful_loans = dl[dl['loan_status'].isin(['Fully Paid'])]

top_affording_job_titles = successful_loans['emp_title'].value_counts().head(2)

print(top_affording_job_titles)

dl.duplicated().sum()

dls = dl['loan_status'].value_counts().sort_values(ascending=False)

dls[0]/(dls[0]+dls[1])

plt.figure(figsize=(7, 5))
dho = dl['home_ownership'].value_counts()
dho.plot(kind='bar')

dl.groupby('grade')['loan_status'].value_counts(normalize=True)

dl['emp_title'].value_counts()

dl.head()

data = dl
data[['mort_acc','emp_title','emp_length','title']].describe([0.05,0.25,0.50,0.75,0.90,0.95,0.99])

data.dropna(inplace = True)

data['emp_title'].value_counts(dropna=False)

data['title'].value_counts(dropna=False)

cols_with_nans = [x for x in data.columns if data[x].isnull().sum() > 200]
#for col in cols_with_nans:
 #   data[col].fillna(data[col].mode()[0], inplace=True)

cols_with_nans

A

#data['mort_acc'] = data['mort_acc'].fillna(data['mort_acc'].mode()[0])
#data['emp_length'] = data['emp_length'].fillna(data['emp_length'].mode()[0])
#data['emp_title'] = data['emp_title'].fillna(data['emp_title'].mode()[0])
#data['title'] = data['title'].fillna(data['title'].mode()[0])

#data['pub_rec_bankruptcies'].dropna(inplace = True)
#data['revol_util'].dropna(inplace = True)
#data['emp_length'] = pd.factorize(data['emp_length'])[0]

#cat_with_null = cols_with_nans
#for cat in cat_with_null:
 #   data[cat] = pd.factorize(data[cat])[0]***

'''for col in cat_with_null:
  nan_ixs = data[col][data[col].isnull()].index
  data['is_nan'] = 0
  data.loc[nan_ixs, 'is_nan'] = 1



  train = data[data['is_nan'] == 0]
  test = data[data['is_nan'] == 1]

  X_train = data.drop(['is_nan'], axis=1)
  y_train = data['is_nan']
  X_test = test.drop(['is_nan'], axis=1)
  model = LGBMClassifier(random_state=0)
  model.fit(X_train, y_train)

  y_pred = model.predict(X_test)
y_pred

freq_imputer = SimpleImputer(strategy = 'most_frequent') # mode
for col in cat_with_null:
    data[col] = pd.DataFrame(freq_imputer.fit_transform(pd.DataFrame(data[col])))
'''

C = data.columns
col_value = pd.DataFrame({
    'Missing Value': data.isnull().sum(),
    'Unique Value': data.nunique(),
    'data types': data.dtypes,
})
col_value.sort_values(by = 'Unique Value',ascending=False).head()

cat_cols  = data[col_value[(col_value['Unique Value'] <=100) | (col_value['data types'] == 'object')].index]
cat_cols.head()

num_cols =  data[col_value[(col_value['Unique Value'] >=100) & (col_value['data types'] != 'object')].index]
num_cols.head()

num_cols.columns

cat_cols.columns

import matplotlib.pyplot as plt
plt.rcParams["figure.figsize"] = [15,10]
num_cols.plot(kind = 'density', subplots = True, layout = (3,3), sharex = False)
plt.show()

plt.figure(figsize=(15, 10))

#Histogram
plt.subplot(2, 4 ,1)
sbn.distplot(num_cols['loan_amnt'], bins = 10)

plt.subplot(2, 4 ,2)
sbn.distplot(num_cols['int_rate'], bins = 10)

plt.subplot(2, 4 ,3)
sbn.distplot(num_cols['annual_inc'], bins = 10)

plt.subplot(2, 4,4)
sbn.distplot(num_cols['dti'], bins = 10)

plt.subplot(2, 4 ,5)
sbn.distplot(num_cols['revol_bal'], bins = 10)

plt.subplot(2, 4 ,6)
sbn.distplot(num_cols['revol_util'], bins = 10)

plt.subplot(2,4 ,7)
sbn.distplot(num_cols['dti'], bins = 10)

import matplotlib.pyplot as plt
plt.rcParams["figure.figsize"] = [15,10]
cat_cols.plot(kind = 'density', subplots = True, layout = (3,3), sharex = False)
plt.show()

cat_cols.columns

#for col in cat_cols:
 # sbn.countplot(data=cat_cols, x=col)

plt.figure(figsize=(15, 10))

#Histogram
plt.subplot(3, 3 ,1)
sbn.countplot(data=cat_cols, x=cat_cols['term'])

plt.subplot(3, 3 ,2)
sbn.countplot(data=cat_cols, x=cat_cols['grade'])

plt.subplot(3, 3 ,3)
sbn.countplot(data=cat_cols, x=cat_cols['loan_status'])

plt.subplot(3, 3,4)
sbn.countplot(data=cat_cols, x=cat_cols['home_ownership'])

plt.subplot(3, 3,5)
sbn.countplot(data=cat_cols, x=cat_cols['pub_rec'])

plt.subplot(3, 3 ,6)
sbn.countplot(data=cat_cols, x=cat_cols['application_type'])

plt.subplot(3,3,7)
sbn.countplot(data=cat_cols, x=cat_cols['verification_status'])

plt.figure(figsize=(10, 5))
sbn.heatmap(num_cols.corr(method='spearman'),
            annot=True, cmap='viridis')
plt.show()

data[['issue_d_month', 'issue_d_year']] = data['issue_d'].str.split('-', expand=True)
data[['ecr_month', 'ecr_year']] = data['earliest_cr_line'].str.split('-', expand=True)
data.drop(['issue_d', 'earliest_cr_line'], axis=1, inplace=True)

num_cols.boxplot(rot=25, figsize=(25,8))

cat_cols.boxplot(rot=25, figsize=(25,8))

# obtain the first quartile
Q1 = num_cols.quantile(0.3)

# obtain the third quartile
Q3 = num_cols.quantile(0.65)

# obtain the IQR
IQR = Q3 - Q1

# print the IQR
IQR

dq = num_cols[~((num_cols < (Q1 - 1.5 * IQR)) |(num_cols > (Q3 + 1.5 * IQR))).any(axis=1)]

dq.boxplot(rot=25, figsize=(25,8))

data = data[~((num_cols < (Q1 - 1.5 * IQR)) |(num_cols > (Q3 + 1.5 * IQR))).any(axis=1)]

"""Ranking: grade, sub_grade,"""

data['sub_grade'].unique()

def extract_state(address):
    # Clean the address by replacing newline characters
    clean_address = address.replace('\r\n', ', ')

    # Split the address to get the last part
    parts = clean_address.split(',')

    if len(parts) > 1:
        # Get the second to last part, which should be the state and ZIP
        state_zip = parts[-1].strip()
        # Split to get state
        state = state_zip.split()[0]  # First part is the state
        return state
    return None

data.isnull().sum()
d3 = data.copy()

data = d3

data['state'] = data['address'].apply(extract_state)

data.head()

#df['make'] = df.groupby('make')['selling_price'].transform('mean')
#df['model'] = df.groupby('model')['selling_price'].transform('mean')

#from sklearn.preprocessing import

grade_order = data['grade'].unique().tolist()
grade_order.sort()
sub_grade_order = data['sub_grade'].unique().tolist()
sub_grade_order.sort()

ordinal_encoder = OrdinalEncoder(categories=[grade_order, sub_grade_order])

data[['grade','sub_grade']] = ordinal_encoder.fit_transform(data[['grade','sub_grade']])

data.isnull().sum().head()

#OH_encoded = col_value[col_value['Unique Value']<=5].index.tolist()

# Apply one-hot encoding
#OHencoder = OneHotEncoder(sparse_output=False)  # Use sparse=True for large datasets
#for i in OH_encoded:
 # one_hot_encoded = OHencoder.fit_transform(data[[i]])
  #one_hot_encoded_df = pd.DataFrame(one_hot_encoded, columns=OHencoder.get_feature_names_out([i]))
  #data = pd.concat([data, one_hot_encoded_df], axis=1)
#data.head()
#data.drop(i, axis=1, inplace=True)

encoded2 = col_value[(col_value['Unique Value']>=5) & (col_value['Unique Value'] <= 30)]#.index.tolist()
col_value.sort_values(by = 'Unique Value',ascending=False)#[col_value['Unique Value']>=5].index.tolist()
ENC = encoded2.index.tolist()
for i in ENC:
  print('Column= '+ i)
  print('Unique value = ',data[i].unique().tolist())
  print('Value count = ',data[i].value_counts(dropna=False))
  print('----------------------------------')

ENC = encoded2.index.tolist()

data['term'].unique().tolist()
data['term'] = data['term'].apply(lambda x: 0 if x == ' 36 months' else 1)

def convert_employee_length(value):
    if '<' in value:  # Handle the "<1 year" case
        return 0
    elif '10+' in value:  # Handle the ">10 years" case
        return 11
    else:
        # Extract the number using regex
        return int(value.split()[0])

data['emp_length'] = data['emp_length'].apply(convert_employee_length)

data['emp_length'].unique()

data['loan_status'].unique()

data['loan_status'] = data['loan_status'].apply(lambda x: 1 if x == 'Fully Paid' else 0 if x == 'Charged Off' else 2 if x == 'Default' else 3)

data['loan_status'].unique()

data['verification_status'].unique()

data['verification_status'] = data['verification_status'].apply(lambda x: 1 if x == 'Verified' else 2 if x == 'Source Verified' else 0)

data['verification_status'].unique()

data

data['purpose'].unique()

data['application_type'].unique()

data['application_type'] = data['application_type'].apply(lambda x: 1 if x == 'INDIVIDUAL' else 0 if x == 'DIRECT_PAY' else 2)

data['application_type'].unique()

data['initial_list_status'].unique()

data['initial_list_status'] = data['initial_list_status'].apply(lambda x: 1 if x == 'w' else 0)

data['initial_list_status'].unique()

data['issue_d_year'].nunique()

data['issue_d_month'].nunique()

data['ecr_year'].nunique()

data['ecr_month'].nunique()

data.head()

target = 'loan_status'
categorical_columns = ['emp_title', 'home_ownership', 'purpose','title', 'address','state', 'ecr_year', 'issue_d_year']  # Add other categorical columns if needed

# Initialize the TargetEncoder
encoder = ce.TargetEncoder(cols=categorical_columns)

# Fit and transform the encoder to your DataFrame
#df_encoded = data.copy()
data[categorical_columns] = encoder.fit_transform(data[categorical_columns], data[target])

data.head()

month_mapping = {
    'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,
    'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12
}
data['issue_d_month'] = data['issue_d_month'].map(month_mapping)
data['ecr_month'] = data['ecr_month'].map(month_mapping)

data.head()

scaler = MinMaxScaler()
df = pd.DataFrame(scaler.fit_transform(data), columns = data.columns)
df.head()

plt.rcParams["figure.figsize"] = [15,10]
df.plot(kind = 'density', subplots = True, layout = (8,5), sharex = False)
plt.show()

print(df.skew())

plt.figure(figsize=(25, 20))
sbn.heatmap(df.corr(method='spearman'),
            annot=True, cmap='viridis')
plt.show()

X = df.drop('loan_status', axis=1)
# Add constant (bias term) for the model
X = add_constant(X)
y = df['loan_status']

vif_data = pd.DataFrame()
vif_data["feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# Display VIF values
print(vif_data)

df = pd.DataFrame(data)

# Separate features and target variable
X = df.drop('loan_status', axis=1)
y = df['loan_status']

# Initialize a logistic regression model
model = LogisticRegression()

# Initialize RFE with the logistic regression model
rfe = RFE(model, n_features_to_select=2)  # Select 2 most important features

# Fit RFE
rfe = rfe.fit(X, y)

# Display selected features
print("Selected Features:", X.columns[rfe.support_])
print("Feature Ranking:", rfe.ranking_)

vif_col = vif_data[vif_data["VIF"]>=5]

vif_col['feature'].tolist()[1:]

df.drop(vif_col['feature'].tolist()[1:], axis=1, inplace=True)

def sigmoid(x):
    return 1/(1+np.e**-x)

# Loss for a single point
def log_loss(y, y_hat):
  gain = y*np.log(y_hat)+(1-y)*np.log(1-y_hat)
  return -gain

cols = df.columns.tolist()
cols.remove('loan_status')
y = df["loan_status"]
y = np.array(y).reshape(len(y), 1) #Reshaping our data to (m,1) shape
X = df[cols]
X.shape

from sklearn.model_selection import train_test_split

X_tr_cv, X_test, y_tr_cv, y_test = train_test_split(X, y, test_size=0.2, random_state=1)
X_train, X_val, y_train, y_val = train_test_split(X_tr_cv, y_tr_cv, test_size=0.25,random_state=1)
X_train.shape

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)

X_train

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)

print("coef = ",model.coef_)
print("intercept = ",model.intercept_)

model.predict(X_train)

def accuracy(y_true, y_pred):
  y_true = y_true.reshape(len(y_true))
  return np.sum(y_true==y_pred)/y_true.shape[0]

accuracy(y_train, model.predict(X_train))

accuracy(y_val, model.predict(X_val))

from sklearn.pipeline import make_pipeline
train_scores = []
val_scores = []
scaler = StandardScaler()
for la in np.arange(0.01, 5000.0, 100): # range of values of Lambda
  scaled_lr = make_pipeline(scaler, LogisticRegression(C=1/la))
  scaled_lr.fit(X_train, y_train)
  train_score = accuracy(y_train, scaled_lr.predict(X_train))
  val_score = accuracy(y_val, scaled_lr.predict(X_val))
  train_scores.append(train_score)
  val_scores.append(val_score)

(val_scores)

plt.figure(figsize=(10,5))
plt.plot(list(np.arange(0.01, 5000.0, 100)), train_scores, label="train")
plt.plot(list(np.arange(0.01, 5000.0, 100)), val_scores, label="val")
plt.legend(loc='lower right')

plt.xlabel("Regularization Parameter(λ)")
plt.ylabel("Accuracy")
plt.grid()
plt.show()

model = LogisticRegression(C=1/1000)

model.fit(X_train, y_train)

print("Train acc = ",accuracy(y_train, model.predict(X_train)))


print("Val acc = ",accuracy(y_val, model.predict(X_val)))

accuracy(y_test, model.predict(X_test))

from sklearn.inspection import DecisionBoundaryDisplay

df.shape

# dataset creation with 3 classes
from sklearn.datasets import make_classification

X, y = make_classification(n_samples= 216113,
                           n_features= 24,
                           n_classes = 2,
                           n_redundant=0,
                           n_clusters_per_class=1,
                           random_state=5)
y=y.reshape(len(y), 1)
plt.scatter(X[:, 0], X[:, 1], c = y)
plt.show()

X_tr_cv, X_test, y_tr_cv, y_test = train_test_split(X, y, test_size=0.2, random_state=4)
X_train, X_val, y_train, y_val = train_test_split(X_tr_cv, y_tr_cv, test_size=0.25,random_state=4)
X_train.shape

model = LogisticRegression(multi_class='ovr')
# fit model
model.fit(X_train, y_train)


print(f'Training Accuracy:{model.score(X_train,y_train)}')
print(f'Validation Accuracy :{model.score(X_val,y_val)}')
print(f'Test Accuracy:{model.score(X_test,y_test)}')

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.model_selection import RandomizedSearchCV
from imblearn.over_sampling import SMOTE
from sklearn.metrics import classification_report

''' X = df.drop('loan_status', axis=1)
y = df['loan_status']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model = RandomForestClassifier()

# Define the hyperparameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
print("Best Hyperparameters:", grid_search.best_params_)

# Make predictions using the best model
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))'''

X = df.drop('loan_status', axis=1)
y = df['loan_status']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)



# Define the model
model = RandomForestClassifier()

# Define the hyperparameter space
param_dist = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'bootstrap': [True, False]
}

# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)

# Fit the model
random_search.fit(X_train, y_train)

# Get the best hyperparameters
print("Best Hyperparameters:", random_search.best_params_)

# Make predictions using the best model
best_model = random_search.best_estimator_
y_pred = best_model.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))

X = df.drop('loan_status', axis=1)
y = df['loan_status']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Define Logistic Regression model
model = LogisticRegression()

# Define the grid of hyperparameters
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],     # Regularization strength (inverse of lambda)
    'penalty': ['l1', 'l2'],          # Regularization type (L1: Lasso, L2: Ridge)
    'solver': ['liblinear', 'saga']    # Solvers compatible with both L1 and L2 penalties
}

# Initialize GridSearchCV with Logistic Regression model and hyperparameter grid
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')

# Fit the grid search
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
print("Best Hyperparameters:", grid_search.best_params_)

# Make predictions using the best model
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))

# Check the distribution of the target variable
print(df['loan_status'].value_counts())

# Visualize the target variable distribution
sbn.countplot(x='loan_status', data=df)
plt.title('Class Distribution of Target Variable')
plt.show()

# Apply SMOTE for oversampling the minority class
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Train a model on the resampled data
model = RandomForestClassifier(random_state=42)
model.fit(X_resampled, y_resampled)

# Predictions and evaluation
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

# Define Logistic Regression model with class_weight='balanced'
model = LogisticRegression(class_weight='balanced', random_state=42)

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print(classification_report(y_test, y_pred))

# Splitting the dataset
X = df.drop('loan_status', axis=1)
y = df['loan_status']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train Logistic Regression model with balanced class weights
model = LogisticRegression(class_weight='balanced', random_state=42)
model.fit(X_train, y_train)

# Extract the coefficients
coefficients = model.coef_[0]  # Get the coefficients for the features
feature_names = X.columns  # Get the feature names

# Create a dataframe to display coefficients with their corresponding feature names
coef_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})

# Sort by the absolute value of the coefficient to see the most important features
coef_df['abs_Coefficient'] = coef_df['Coefficient'].abs()
coef_df = coef_df.sort_values(by='abs_Coefficient', ascending=False)

# Display the coefficients
print(coef_df[['Feature', 'Coefficient']])

# Splitting the dataset
X = df.drop('loan_status', axis=1)
y = df['loan_status']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train Logistic Regression model with balanced class weights
model = LogisticRegression(class_weight='balanced', random_state=42)
model.fit(X_train, y_train)

# Extract the coefficients
coefficients = model.coef_[0]  # Get the coefficients for the features
feature_names = X.columns  # Get the feature names

# Create a dataframe to display coefficients with their corresponding feature names
coef_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})

# Calculate the odds ratio
coef_df['Odds Ratio'] = np.exp(coef_df['Coefficient'])

# Sort by the absolute value of the coefficient to see the most important features
coef_df['abs_Coefficient'] = coef_df['Coefficient'].abs()
coef_df = coef_df.sort_values(by='abs_Coefficient', ascending=False)

# Display the coefficients and odds ratio
print(coef_df[['Feature', 'Coefficient', 'Odds Ratio']])

from sklearn.metrics import roc_curve, roc_auc_score

# Assuming model is already trained and X_test, y_test are available
y_pred_proba = model.predict_proba(X_test)[:, 1]  # Get predicted probabilities

# Calculate the ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)

# Plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc_score(y_test, y_pred_proba):.2f})')
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')  # Line for random model
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

from sklearn.metrics import precision_recall_curve, average_precision_score

# Calculate precision-recall curve
precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)

# Plot the precision-recall curve
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, label=f'Precision-Recall curve (AP = {average_precision_score(y_test, y_pred_proba):.2f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.grid(True)
plt.show()

# Sort the coefficients by their absolute values
coef_df = coef_df.sort_values(by='abs_Coefficient', ascending=False)

# Plot feature importance
plt.figure(figsize=(10, 6))
plt.barh(coef_df['Feature'], coef_df['Coefficient'], color='skyblue')
plt.xlabel('Coefficient Value')
plt.title('Feature Importance (Logistic Regression Coefficients)')
plt.grid(True)
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Create confusion matrix
cm = confusion_matrix(y_test, model.predict(X_test))

# Plot confusion matrix
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

from sklearn.metrics import confusion_matrix

# Predicted probabilities
y_pred_proba = model.predict_proba(X_test)[:, 1]

# Adjust threshold (e.g., 0.3 instead of 0.5)
threshold = 0.3
y_pred_adjusted = (y_pred_proba >= threshold).astype(int)

# Confusion matrix for adjusted threshold
cm_adjusted = confusion_matrix(y_test, y_pred_adjusted)

print(cm_adjusted)

model = LogisticRegression(class_weight={0: 1, 1: 2})  # 2x more importance on false positives
model.fit(X_train, y_train)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, precision_recall_curve

# ROC curve
fpr, tpr, roc_thresholds = roc_curve(y_test, y_pred_proba)
plt.plot(roc_thresholds, fpr, label='False Positive Rate')
plt.plot(roc_thresholds, tpr, label='True Positive Rate')
plt.xlabel('Threshold')
plt.ylabel('Rate')
plt.legend()
plt.title('ROC Curve - Thresholds vs FPR/TPR')
plt.show()

# Precision-Recall Curve
precision, recall, pr_thresholds = precision_recall_curve(y_test, y_pred_proba)
plt.plot(pr_thresholds, precision[:-1], label='Precision')
plt.plot(pr_thresholds, recall[:-1], label='Recall')
plt.xlabel('Threshold')
plt.ylabel('Score')
plt.legend()
plt.title('Precision-Recall Curve - Thresholds vs Precision/Recall')
plt.show()

"""Questionnaire (Answers should present in the text editor along with insights):
1. What percentage of customers have fully paid their Loan Amount?

  =>80.36 %

2. Comment about the correlation between Loan Amount and Installment features.

  => Both are highly correlated, with correlation value of 0.96

3. The majority of people have home ownership as .

  => MORTGAGE

4. People with grades ‘A’ are more likely to fully pay their loan. (T/F)

  =>True

5. Name the top 2 afforded job titles.
  => Teacher and Manager

6. Thinking from a bank's perspective, which metric should our primary focus be on..
  => Recall and F1 Score
7. How does the gap in precision and recall affect the bank?

  => The gap between precision and recall highlights the bank's approach to managing risk versus seizing growth opportunities. A bank should decide based on its strategic objectives—whether it prioritizes safety or expansion.
8. Which were the features that heavily affected the outcome?

  => Open_Account, dti, verification status, issue_d_month

9. Will the results be affected by geographical location?

  => No

"""

